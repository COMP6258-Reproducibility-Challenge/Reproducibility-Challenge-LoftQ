Running SLURM prolog script on pink58.cluster.local
===============================================================================
Job started on Wed Apr 30 13:14:47 BST 2025
Job ID          : 7348485
Job name        : q_t_iridis.sh
WorkDir         : /mainfs/lyceum/cjm1n19/LoftQ/custom_loftq
Command         : ./scripts/q_t_iridis.sh
Partition       : lyceum
Num hosts       : 1
Num cores       : 16
Num of tasks    : 1
Hosts allocated : pink58
Job Output Follows ...
===============================================================================
[2025-04-30 13:16:18,943] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
WARNING:root:Loading raw dataset
Using the latest cached version of the dataset since glue couldn't be found on the Hugging Face Hub
WARNING:datasets.load:Using the latest cached version of the dataset since glue couldn't be found on the Hugging Face Hub
Found the latest cached dataset configuration 'rte' at /scratch/cjm1n19/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c (last modified on Wed Apr 30 13:11:58 2025).
WARNING:datasets.packaged_modules.cache.cache:Found the latest cached dataset configuration 'rte' at /scratch/cjm1n19/huggingface/datasets/glue/rte/0.0.0/bcdcba79d07bc864c1c254ccfcedcce55bcc9a8c (last modified on Wed Apr 30 13:11:58 2025).
Some weights of the model checkpoint at quantized_models/deberta-v3-base-4bit-16rank were not used when initializing DebertaV2ForSequenceClassification: ['deberta.encoder.layer.0.attention.output.dense.base_layer.bias', 'deberta.encoder.layer.0.attention.output.dense.base_layer.weight', 'deberta.encoder.layer.0.attention.output.dense.lora_A.weight', 'deberta.encoder.layer.0.attention.output.dense.lora_B.weight', 'deberta.encoder.layer.0.attention.self.key_proj.base_layer.bias', 'deberta.encoder.layer.0.attention.self.key_proj.base_layer.weight', 'deberta.encoder.layer.0.attention.self.key_proj.lora_A.weight', 'deberta.encoder.layer.0.attention.self.key_proj.lora_B.weight', 'deberta.encoder.layer.0.attention.self.query_proj.base_layer.bias', 'deberta.encoder.layer.0.attention.self.query_proj.base_layer.weight', 'deberta.encoder.layer.0.attention.self.query_proj.lora_A.weight', 'deberta.encoder.layer.0.attention.self.query_proj.lora_B.weight', 'deberta.encoder.layer.0.attention.self.value_proj.base_layer.bias', 'deberta.encoder.layer.0.attention.self.value_proj.base_layer.weight', 'deberta.encoder.layer.0.attention.self.value_proj.lora_A.weight', 'deberta.encoder.layer.0.attention.self.value_proj.lora_B.weight', 'deberta.encoder.layer.0.intermediate.dense.base_layer.bias', 'deberta.encoder.layer.0.intermediate.dense.base_layer.weight', 'deberta.encoder.layer.0.intermediate.dense.lora_A.weight', 'deberta.encoder.layer.0.intermediate.dense.lora_B.weight', 'deberta.encoder.layer.0.output.dense.base_layer.bias', 'deberta.encoder.layer.0.output.dense.base_layer.weight', 'deberta.encoder.layer.0.output.dense.lora_A.weight', 'deberta.encoder.layer.0.output.dense.lora_B.weight', 'deberta.encoder.layer.1.attention.output.dense.base_layer.bias', 'deberta.encoder.layer.1.attention.output.dense.base_layer.weight', 'deberta.encoder.layer.1.attention.output.dense.lora_A.weight', 'deberta.encoder.layer.1.attention.output.dense.lora_B.weight', 'deberta.encoder.layer.1.attention.self.key_proj.base_layer.bias', 'deberta.encoder.layer.1.attention.self.key_proj.base_layer.weight', 'deberta.encoder.layer.1.attention.self.key_proj.lora_A.weight', 'deberta.encoder.layer.1.attention.self.key_proj.lora_B.weight', 'deberta.encoder.layer.1.attention.self.query_proj.base_layer.bias', 'deberta.encoder.layer.1.attention.self.query_proj.base_layer.weight', 'deberta.encoder.layer.1.attention.self.query_proj.lora_A.weight', 'deberta.encoder.layer.1.attention.self.query_proj.lora_B.weight', 'deberta.encoder.layer.1.attention.self.value_proj.base_layer.bias', 'deberta.encoder.layer.1.attention.self.value_proj.base_layer.weight', 'deberta.encoder.layer.1.attention.self.value_proj.lora_A.weight', 'deberta.encoder.layer.1.attention.self.value_proj.lora_B.weight', 'deberta.encoder.layer.1.intermediate.dense.base_layer.bias', 'deberta.encoder.layer.1.intermediate.dense.base_layer.weight', 'deberta.encoder.layer.1.intermediate.dense.lora_A.weight', 'deberta.encoder.layer.1.intermediate.dense.lora_B.weight', 'deberta.encoder.layer.1.output.dense.base_layer.bias', 'deberta.encoder.layer.1.output.dense.base_layer.weight', 'deberta.encoder.layer.1.output.dense.lora_A.weight', 'deberta.encoder.layer.1.output.dense.lora_B.weight', 'deberta.encoder.layer.10.attention.output.dense.base_layer.bias', 'deberta.encoder.layer.10.attention.output.dense.base_layer.weight', 'deberta.encoder.layer.10.attention.output.dense.lora_A.weight', 'deberta.encoder.layer.10.attention.output.dense.lora_B.weight', 'deberta.encoder.layer.10.attention.self.key_proj.base_layer.bias', 'deberta.encoder.layer.10.attention.self.key_proj.base_layer.weight', 'deberta.encoder.layer.10.attention.self.key_proj.lora_A.weight', 'deberta.encoder.layer.10.attention.self.key_proj.lora_B.weight', 'deberta.encoder.layer.10.attention.self.query_proj.base_layer.bias', 'deberta.encoder.layer.10.attention.self.query_proj.base_layer.weight', 'deberta.encoder.layer.10.attention.self.query_proj.lora_A.weight', 'deberta.encoder.layer.10.attention.self.query_proj.lora_B.weight', 'deberta.encoder.layer.10.attention.self.value_proj.base_layer.bias', 'deberta.encoder.layer.10.attention.self.value_proj.base_layer.weight', 'deberta.encoder.layer.10.attention.self.value_proj.lora_A.weight', 'deberta.encoder.layer.10.attention.self.value_proj.lora_B.weight', 'deberta.encoder.layer.10.intermediate.dense.base_layer.bias', 'deberta.encoder.layer.10.intermediate.dense.base_layer.weight', 'deberta.encoder.layer.10.intermediate.dense.lora_A.weight', 'deberta.encoder.layer.10.intermediate.dense.lora_B.weight', 'deberta.encoder.layer.10.output.dense.base_layer.bias', 'deberta.encoder.layer.10.output.dense.base_layer.weight', 'deberta.encoder.layer.10.output.dense.lora_A.weight', 'deberta.encoder.layer.10.output.dense.lora_B.weight', 'deberta.encoder.layer.11.attention.output.dense.base_layer.bias', 'deberta.encoder.layer.11.attention.output.dense.base_layer.weight', 'deberta.encoder.layer.11.attention.output.dense.lora_A.weight', 'deberta.encoder.layer.11.attention.output.dense.lora_B.weight', 'deberta.encoder.layer.11.attention.self.key_proj.base_layer.bias', 'deberta.encoder.layer.11.attention.self.key_proj.base_layer.weight', 'deberta.encoder.layer.11.attention.self.key_proj.lora_A.weight', 'deberta.encoder.layer.11.attention.self.key_proj.lora_B.weight', 'deberta.encoder.layer.11.attention.self.query_proj.base_layer.bias', 'deberta.encoder.layer.11.attention.self.query_proj.base_layer.weight', 'deberta.encoder.layer.11.attention.self.query_proj.lora_A.weight', 'deberta.encoder.layer.11.attention.self.query_proj.lora_B.weight', 'deberta.encoder.layer.11.attention.self.value_proj.base_layer.bias', 'deberta.encoder.layer.11.attention.self.value_proj.base_layer.weight', 'deberta.encoder.layer.11.attention.self.value_proj.lora_A.weight', 'deberta.encoder.layer.11.attention.self.value_proj.lora_B.weight', 'deberta.encoder.layer.11.intermediate.dense.base_layer.bias', 'deberta.encoder.layer.11.intermediate.dense.base_layer.weight', 'deberta.encoder.layer.11.intermediate.dense.lora_A.weight', 'deberta.encoder.layer.11.intermediate.dense.lora_B.weight', 'deberta.encoder.layer.11.output.dense.base_layer.bias', 'deberta.encoder.layer.11.output.dense.base_layer.weight', 'deberta.encoder.layer.11.output.dense.lora_A.weight', 'deberta.encoder.layer.11.output.dense.lora_B.weight', 'deberta.encoder.layer.2.attention.output.dense.base_layer.bias', 'deberta.encoder.layer.2.attention.output.dense.base_layer.weight', 'deberta.encoder.layer.2.attention.output.dense.lora_A.weight', 'deberta.encoder.layer.2.attention.output.dense.lora_B.weight', 'deberta.encoder.layer.2.attention.self.key_proj.base_layer.bias', 'deberta.encoder.layer.2.attention.self.key_proj.base_layer.weight', 'deberta.encoder.layer.2.attention.self.key_proj.lora_A.weight', 'deberta.encoder.layer.2.attention.self.key_proj.lora_B.weight', 'deberta.encoder.layer.2.attention.self.query_proj.base_layer.bias', 'deberta.encoder.layer.2.attention.self.query_proj.base_layer.weight', 'deberta.encoder.layer.2.attention.self.query_proj.lora_A.weight', 'deberta.encoder.layer.2.attention.self.query_proj.lora_B.weight', 'deberta.encoder.layer.2.attention.self.value_proj.base_layer.bias', 'deberta.encoder.layer.2.attention.self.value_proj.base_layer.weight', 'deberta.encoder.layer.2.attention.self.value_proj.lora_A.weight', 'deberta.encoder.layer.2.attention.self.value_proj.lora_B.weight', 'deberta.encoder.layer.2.intermediate.dense.base_layer.bias', 'deberta.encoder.layer.2.intermediate.dense.base_layer.weight', 'deberta.encoder.layer.2.intermediate.dense.lora_A.weight', 'deberta.encoder.layer.2.intermediate.dense.lora_B.weight', 'deberta.encoder.layer.2.output.dense.base_layer.bias', 'deberta.encoder.layer.2.output.dense.base_layer.weight', 'deberta.encoder.layer.2.output.dense.lora_A.weight', 'deberta.encoder.layer.2.output.dense.lora_B.weight', 'deberta.encoder.layer.3.attention.output.dense.base_layer.bias', 'deberta.encoder.layer.3.attention.output.dense.base_layer.weight', 'deberta.encoder.layer.3.attention.output.dense.lora_A.weight', 'deberta.encoder.layer.3.attention.output.dense.lora_B.weight', 'deberta.encoder.layer.3.attention.self.key_proj.base_layer.bias', 'deberta.encoder.layer.3.attention.self.key_proj.base_layer.weight', 'deberta.encoder.layer.3.attention.self.key_proj.lora_A.weight', 'deberta.encoder.layer.3.attention.self.key_proj.lora_B.weight', 'deberta.encoder.layer.3.attention.self.query_proj.base_layer.bias', 'deberta.encoder.layer.3.attention.self.query_proj.base_layer.weight', 'deberta.encoder.layer.3.attention.self.query_proj.lora_A.weight', 'deberta.encoder.layer.3.attention.self.query_proj.lora_B.weight', 'deberta.encoder.layer.3.attention.self.value_proj.base_layer.bias', 'deberta.encoder.layer.3.attention.self.value_proj.base_layer.weight', 'deberta.encoder.layer.3.attention.self.value_proj.lora_A.weight', 'deberta.encoder.layer.3.attention.self.value_proj.lora_B.weight', 'deberta.encoder.layer.3.intermediate.dense.base_layer.bias', 'deberta.encoder.layer.3.intermediate.dense.base_layer.weight', 'deberta.encoder.layer.3.intermediate.dense.lora_A.weight', 'deberta.encoder.layer.3.intermediate.dense.lora_B.weight', 'deberta.encoder.layer.3.output.dense.base_layer.bias', 'deberta.encoder.layer.3.output.dense.base_layer.weight', 'deberta.encoder.layer.3.output.dense.lora_A.weight', 'deberta.encoder.layer.3.output.dense.lora_B.weight', 'deberta.encoder.layer.4.attention.output.dense.base_layer.bias', 'deberta.encoder.layer.4.attention.output.dense.base_layer.weight', 'deberta.encoder.layer.4.attention.output.dense.lora_A.weight', 'deberta.encoder.layer.4.attention.output.dense.lora_B.weight', 'deberta.encoder.layer.4.attention.self.key_proj.base_layer.bias', 'deberta.encoder.layer.4.attention.self.key_proj.base_layer.weight', 'deberta.encoder.layer.4.attention.self.key_proj.lora_A.weight', 'deberta.encoder.layer.4.attention.self.key_proj.lora_B.weight', 'deberta.encoder.layer.4.attention.self.query_proj.base_layer.bias', 'deberta.encoder.layer.4.attention.self.query_proj.base_layer.weight', 'deberta.encoder.layer.4.attention.self.query_proj.lora_A.weight', 'deberta.encoder.layer.4.attention.self.query_proj.lora_B.weight', 'deberta.encoder.layer.4.attention.self.value_proj.base_layer.bias', 'deberta.encoder.layer.4.attention.self.value_proj.base_layer.weight', 'deberta.encoder.layer.4.attention.self.value_proj.lora_A.weight', 'deberta.encoder.layer.4.attention.self.value_proj.lora_B.weight', 'deberta.encoder.layer.4.intermediate.dense.base_layer.bias', 'deberta.encoder.layer.4.intermediate.dense.base_layer.weight', 'deberta.encoder.layer.4.intermediate.dense.lora_A.weight', 'deberta.encoder.layer.4.intermediate.dense.lora_B.weight', 'deberta.encoder.layer.4.output.dense.base_layer.bias', 'deberta.encoder.layer.4.output.dense.base_layer.weight', 'deberta.encoder.layer.4.output.dense.lora_A.weight', 'deberta.encoder.layer.4.output.dense.lora_B.weight', 'deberta.encoder.layer.5.attention.output.dense.base_layer.bias', 'deberta.encoder.layer.5.attention.output.dense.base_layer.weight', 'deberta.encoder.layer.5.attention.output.dense.lora_A.weight', 'deberta.encoder.layer.5.attention.output.dense.lora_B.weight', 'deberta.encoder.layer.5.attention.self.key_proj.base_layer.bias', 'deberta.encoder.layer.5.attention.self.key_proj.base_layer.weight', 'deberta.encoder.layer.5.attention.self.key_proj.lora_A.weight', 'deberta.encoder.layer.5.attention.self.key_proj.lora_B.weight', 'deberta.encoder.layer.5.attention.self.query_proj.base_layer.bias', 'deberta.encoder.layer.5.attention.self.query_proj.base_layer.weight', 'deberta.encoder.layer.5.attention.self.query_proj.lora_A.weight', 'deberta.encoder.layer.5.attention.self.query_proj.lora_B.weight', 'deberta.encoder.layer.5.attention.self.value_proj.base_layer.bias', 'deberta.encoder.layer.5.attention.self.value_proj.base_layer.weight', 'deberta.encoder.layer.5.attention.self.value_proj.lora_A.weight', 'deberta.encoder.layer.5.attention.self.value_proj.lora_B.weight', 'deberta.encoder.layer.5.intermediate.dense.base_layer.bias', 'deberta.encoder.layer.5.intermediate.dense.base_layer.weight', 'deberta.encoder.layer.5.intermediate.dense.lora_A.weight', 'deberta.encoder.layer.5.intermediate.dense.lora_B.weight', 'deberta.encoder.layer.5.output.dense.base_layer.bias', 'deberta.encoder.layer.5.output.dense.base_layer.weight', 'deberta.encoder.layer.5.output.dense.lora_A.weight', 'deberta.encoder.layer.5.output.dense.lora_B.weight', 'deberta.encoder.layer.6.attention.output.dense.base_layer.bias', 'deberta.encoder.layer.6.attention.output.dense.base_layer.weight', 'deberta.encoder.layer.6.attention.output.dense.lora_A.weight', 'deberta.encoder.layer.6.attention.output.dense.lora_B.weight', 'deberta.encoder.layer.6.attention.self.key_proj.base_layer.bias', 'deberta.encoder.layer.6.attention.self.key_proj.base_layer.weight', 'deberta.encoder.layer.6.attention.self.key_proj.lora_A.weight', 'deberta.encoder.layer.6.attention.self.key_proj.lora_B.weight', 'deberta.encoder.layer.6.attention.self.query_proj.base_layer.bias', 'deberta.encoder.layer.6.attention.self.query_proj.base_layer.weight', 'deberta.encoder.layer.6.attention.self.query_proj.lora_A.weight', 'deberta.encoder.layer.6.attention.self.query_proj.lora_B.weight', 'deberta.encoder.layer.6.attention.self.value_proj.base_layer.bias', 'deberta.encoder.layer.6.attention.self.value_proj.base_layer.weight', 'deberta.encoder.layer.6.attention.self.value_proj.lora_A.weight', 'deberta.encoder.layer.6.attention.self.value_proj.lora_B.weight', 'deberta.encoder.layer.6.intermediate.dense.base_layer.bias', 'deberta.encoder.layer.6.intermediate.dense.base_layer.weight', 'deberta.encoder.layer.6.intermediate.dense.lora_A.weight', 'deberta.encoder.layer.6.intermediate.dense.lora_B.weight', 'deberta.encoder.layer.6.output.dense.base_layer.bias', 'deberta.encoder.layer.6.output.dense.base_layer.weight', 'deberta.encoder.layer.6.output.dense.lora_A.weight', 'deberta.encoder.layer.6.output.dense.lora_B.weight', 'deberta.encoder.layer.7.attention.output.dense.base_layer.bias', 'deberta.encoder.layer.7.attention.output.dense.base_layer.weight', 'deberta.encoder.layer.7.attention.output.dense.lora_A.weight', 'deberta.encoder.layer.7.attention.output.dense.lora_B.weight', 'deberta.encoder.layer.7.attention.self.key_proj.base_layer.bias', 'deberta.encoder.layer.7.attention.self.key_proj.base_layer.weight', 'deberta.encoder.layer.7.attention.self.key_proj.lora_A.weight', 'deberta.encoder.layer.7.attention.self.key_proj.lora_B.weight', 'deberta.encoder.layer.7.attention.self.query_proj.base_layer.bias', 'deberta.encoder.layer.7.attention.self.query_proj.base_layer.weight', 'deberta.encoder.layer.7.attention.self.query_proj.lora_A.weight', 'deberta.encoder.layer.7.attention.self.query_proj.lora_B.weight', 'deberta.encoder.layer.7.attention.self.value_proj.base_layer.bias', 'deberta.encoder.layer.7.attention.self.value_proj.base_layer.weight', 'deberta.encoder.layer.7.attention.self.value_proj.lora_A.weight', 'deberta.encoder.layer.7.attention.self.value_proj.lora_B.weight', 'deberta.encoder.layer.7.intermediate.dense.base_layer.bias', 'deberta.encoder.layer.7.intermediate.dense.base_layer.weight', 'deberta.encoder.layer.7.intermediate.dense.lora_A.weight', 'deberta.encoder.layer.7.intermediate.dense.lora_B.weight', 'deberta.encoder.layer.7.output.dense.base_layer.bias', 'deberta.encoder.layer.7.output.dense.base_layer.weight', 'deberta.encoder.layer.7.output.dense.lora_A.weight', 'deberta.encoder.layer.7.output.dense.lora_B.weight', 'deberta.encoder.layer.8.attention.output.dense.base_layer.bias', 'deberta.encoder.layer.8.attention.output.dense.base_layer.weight', 'deberta.encoder.layer.8.attention.output.dense.lora_A.weight', 'deberta.encoder.layer.8.attention.output.dense.lora_B.weight', 'deberta.encoder.layer.8.attention.self.key_proj.base_layer.bias', 'deberta.encoder.layer.8.attention.self.key_proj.base_layer.weight', 'deberta.encoder.layer.8.attention.self.key_proj.lora_A.weight', 'deberta.encoder.layer.8.attention.self.key_proj.lora_B.weight', 'deberta.encoder.layer.8.attention.self.query_proj.base_layer.bias', 'deberta.encoder.layer.8.attention.self.query_proj.base_layer.weight', 'deberta.encoder.layer.8.attention.self.query_proj.lora_A.weight', 'deberta.encoder.layer.8.attention.self.query_proj.lora_B.weight', 'deberta.encoder.layer.8.attention.self.value_proj.base_layer.bias', 'deberta.encoder.layer.8.attention.self.value_proj.base_layer.weight', 'deberta.encoder.layer.8.attention.self.value_proj.lora_A.weight', 'deberta.encoder.layer.8.attention.self.value_proj.lora_B.weight', 'deberta.encoder.layer.8.intermediate.dense.base_layer.bias', 'deberta.encoder.layer.8.intermediate.dense.base_layer.weight', 'deberta.encoder.layer.8.intermediate.dense.lora_A.weight', 'deberta.encoder.layer.8.intermediate.dense.lora_B.weight', 'deberta.encoder.layer.8.output.dense.base_layer.bias', 'deberta.encoder.layer.8.output.dense.base_layer.weight', 'deberta.encoder.layer.8.output.dense.lora_A.weight', 'deberta.encoder.layer.8.output.dense.lora_B.weight', 'deberta.encoder.layer.9.attention.output.dense.base_layer.bias', 'deberta.encoder.layer.9.attention.output.dense.base_layer.weight', 'deberta.encoder.layer.9.attention.output.dense.lora_A.weight', 'deberta.encoder.layer.9.attention.output.dense.lora_B.weight', 'deberta.encoder.layer.9.attention.self.key_proj.base_layer.bias', 'deberta.encoder.layer.9.attention.self.key_proj.base_layer.weight', 'deberta.encoder.layer.9.attention.self.key_proj.lora_A.weight', 'deberta.encoder.layer.9.attention.self.key_proj.lora_B.weight', 'deberta.encoder.layer.9.attention.self.query_proj.base_layer.bias', 'deberta.encoder.layer.9.attention.self.query_proj.base_layer.weight', 'deberta.encoder.layer.9.attention.self.query_proj.lora_A.weight', 'deberta.encoder.layer.9.attention.self.query_proj.lora_B.weight', 'deberta.encoder.layer.9.attention.self.value_proj.base_layer.bias', 'deberta.encoder.layer.9.attention.self.value_proj.base_layer.weight', 'deberta.encoder.layer.9.attention.self.value_proj.lora_A.weight', 'deberta.encoder.layer.9.attention.self.value_proj.lora_B.weight', 'deberta.encoder.layer.9.intermediate.dense.base_layer.bias', 'deberta.encoder.layer.9.intermediate.dense.base_layer.weight', 'deberta.encoder.layer.9.intermediate.dense.lora_A.weight', 'deberta.encoder.layer.9.intermediate.dense.lora_B.weight', 'deberta.encoder.layer.9.output.dense.base_layer.bias', 'deberta.encoder.layer.9.output.dense.base_layer.weight', 'deberta.encoder.layer.9.output.dense.lora_A.weight', 'deberta.encoder.layer.9.output.dense.lora_B.weight']
- This IS expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DebertaV2ForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at quantized_models/deberta-v3-base-4bit-16rank and are newly initialized: ['deberta.encoder.layer.0.attention.output.dense.weight', 'deberta.encoder.layer.0.attention.self.key_proj.weight', 'deberta.encoder.layer.0.attention.self.query_proj.weight', 'deberta.encoder.layer.0.attention.self.value_proj.weight', 'deberta.encoder.layer.0.intermediate.dense.weight', 'deberta.encoder.layer.0.output.dense.weight', 'deberta.encoder.layer.1.attention.output.dense.weight', 'deberta.encoder.layer.1.attention.self.key_proj.weight', 'deberta.encoder.layer.1.attention.self.query_proj.weight', 'deberta.encoder.layer.1.attention.self.value_proj.weight', 'deberta.encoder.layer.1.intermediate.dense.weight', 'deberta.encoder.layer.1.output.dense.weight', 'deberta.encoder.layer.10.attention.output.dense.weight', 'deberta.encoder.layer.10.attention.self.key_proj.weight', 'deberta.encoder.layer.10.attention.self.query_proj.weight', 'deberta.encoder.layer.10.attention.self.value_proj.weight', 'deberta.encoder.layer.10.intermediate.dense.weight', 'deberta.encoder.layer.10.output.dense.weight', 'deberta.encoder.layer.11.attention.output.dense.weight', 'deberta.encoder.layer.11.attention.self.key_proj.weight', 'deberta.encoder.layer.11.attention.self.query_proj.weight', 'deberta.encoder.layer.11.attention.self.value_proj.weight', 'deberta.encoder.layer.11.intermediate.dense.weight', 'deberta.encoder.layer.11.output.dense.weight', 'deberta.encoder.layer.2.attention.output.dense.weight', 'deberta.encoder.layer.2.attention.self.key_proj.weight', 'deberta.encoder.layer.2.attention.self.query_proj.weight', 'deberta.encoder.layer.2.attention.self.value_proj.weight', 'deberta.encoder.layer.2.intermediate.dense.weight', 'deberta.encoder.layer.2.output.dense.weight', 'deberta.encoder.layer.3.attention.output.dense.weight', 'deberta.encoder.layer.3.attention.self.key_proj.weight', 'deberta.encoder.layer.3.attention.self.query_proj.weight', 'deberta.encoder.layer.3.attention.self.value_proj.weight', 'deberta.encoder.layer.3.intermediate.dense.weight', 'deberta.encoder.layer.3.output.dense.weight', 'deberta.encoder.layer.4.attention.output.dense.weight', 'deberta.encoder.layer.4.attention.self.key_proj.weight', 'deberta.encoder.layer.4.attention.self.query_proj.weight', 'deberta.encoder.layer.4.attention.self.value_proj.weight', 'deberta.encoder.layer.4.intermediate.dense.weight', 'deberta.encoder.layer.4.output.dense.weight', 'deberta.encoder.layer.5.attention.output.dense.weight', 'deberta.encoder.layer.5.attention.self.key_proj.weight', 'deberta.encoder.layer.5.attention.self.query_proj.weight', 'deberta.encoder.layer.5.attention.self.value_proj.weight', 'deberta.encoder.layer.5.intermediate.dense.weight', 'deberta.encoder.layer.5.output.dense.weight', 'deberta.encoder.layer.6.attention.output.dense.weight', 'deberta.encoder.layer.6.attention.self.key_proj.weight', 'deberta.encoder.layer.6.attention.self.query_proj.weight', 'deberta.encoder.layer.6.attention.self.value_proj.weight', 'deberta.encoder.layer.6.intermediate.dense.weight', 'deberta.encoder.layer.6.output.dense.weight', 'deberta.encoder.layer.7.attention.output.dense.weight', 'deberta.encoder.layer.7.attention.self.key_proj.weight', 'deberta.encoder.layer.7.attention.self.query_proj.weight', 'deberta.encoder.layer.7.attention.self.value_proj.weight', 'deberta.encoder.layer.7.intermediate.dense.weight', 'deberta.encoder.layer.7.output.dense.weight', 'deberta.encoder.layer.8.attention.output.dense.weight', 'deberta.encoder.layer.8.attention.self.key_proj.weight', 'deberta.encoder.layer.8.attention.self.query_proj.weight', 'deberta.encoder.layer.8.attention.self.value_proj.weight', 'deberta.encoder.layer.8.intermediate.dense.weight', 'deberta.encoder.layer.8.output.dense.weight', 'deberta.encoder.layer.9.attention.output.dense.weight', 'deberta.encoder.layer.9.attention.self.key_proj.weight', 'deberta.encoder.layer.9.attention.self.query_proj.weight', 'deberta.encoder.layer.9.attention.self.value_proj.weight', 'deberta.encoder.layer.9.intermediate.dense.weight', 'deberta.encoder.layer.9.output.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
/lyceum/cjm1n19/.conda/envs/comp6258_env/lib/python3.12/site-packages/transformers/convert_slow_tokenizer.py:559: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.
  warnings.warn(
WARNING:root:Preparing to train model
LoftQ - Total parameters: 187,161,603
LoftQ - Trainable parameters: 3,247,107 (1.73%)
Map:   0%|          | 0/2490 [00:00<?, ? examples/s]Map:  40%|████      | 1000/2490 [00:00<00:00, 5025.83 examples/s]Map:  80%|████████  | 2000/2490 [00:00<00:00, 5702.14 examples/s]Map: 100%|██████████| 2490/2490 [00:00<00:00, 5591.54 examples/s]
Map:   0%|          | 0/277 [00:00<?, ? examples/s]Map: 100%|██████████| 277/277 [00:00<00:00, 5320.99 examples/s]
Map:   0%|          | 0/3000 [00:00<?, ? examples/s]Map:  33%|███▎      | 1000/3000 [00:00<00:00, 6797.08 examples/s]Map:  67%|██████▋   | 2000/3000 [00:00<00:00, 6487.91 examples/s]Map: 100%|██████████| 3000/3000 [00:00<00:00, 6098.87 examples/s]Map: 100%|██████████| 3000/3000 [00:00<00:00, 6086.78 examples/s]
WARNING:accelerate.utils.other:Detected kernel version 3.10.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
Training model...
  0%|          | 0/234 [00:00<?, ?it/s]  0%|          | 1/234 [00:04<16:18,  4.20s/it]  1%|          | 2/234 [00:05<08:47,  2.27s/it]  1%|▏         | 3/234 [00:06<06:22,  1.66s/it]  2%|▏         | 4/234 [00:06<05:14,  1.37s/it]  2%|▏         | 5/234 [00:07<04:36,  1.21s/it]  3%|▎         | 6/234 [00:08<04:13,  1.11s/it]  3%|▎         | 7/234 [00:09<03:58,  1.05s/it]  3%|▎         | 8/234 [00:10<03:48,  1.01s/it]  4%|▍         | 9/234 [00:11<03:41,  1.02it/s]  4%|▍         | 10/234 [00:12<03:36,  1.04it/s]  5%|▍         | 11/234 [00:13<03:32,  1.05it/s]  5%|▌         | 12/234 [00:14<03:29,  1.06it/s]  6%|▌         | 13/234 [00:15<03:27,  1.06it/s]  6%|▌         | 14/234 [00:16<03:25,  1.07it/s]  6%|▋         | 15/234 [00:17<03:24,  1.07it/s]  7%|▋         | 16/234 [00:18<03:23,  1.07it/s]  7%|▋         | 17/234 [00:19<03:21,  1.08it/s]  8%|▊         | 18/234 [00:19<03:20,  1.08it/s]  8%|▊         | 19/234 [00:20<03:19,  1.08it/s]  9%|▊         | 20/234 [00:21<03:18,  1.08it/s]  9%|▉         | 21/234 [00:22<03:17,  1.08it/s]  9%|▉         | 22/234 [00:23<03:16,  1.08it/s] 10%|▉         | 23/234 [00:24<03:15,  1.08it/s] 10%|█         | 24/234 [00:25<03:14,  1.08it/s] 11%|█         | 25/234 [00:26<03:13,  1.08it/s] 11%|█         | 26/234 [00:27<03:12,  1.08it/s] 12%|█▏        | 27/234 [00:28<03:11,  1.08it/s] 12%|█▏        | 28/234 [00:29<03:11,  1.08it/s] 12%|█▏        | 29/234 [00:30<03:10,  1.08it/s] 13%|█▎        | 30/234 [00:31<03:09,  1.08it/s] 13%|█▎        | 31/234 [00:31<03:08,  1.08it/s] 14%|█▎        | 32/234 [00:32<03:07,  1.08it/s] 14%|█▍        | 33/234 [00:33<03:06,  1.08it/s] 15%|█▍        | 34/234 [00:34<03:05,  1.08it/s] 15%|█▍        | 35/234 [00:35<03:04,  1.08it/s] 15%|█▌        | 36/234 [00:36<03:03,  1.08it/s] 16%|█▌        | 37/234 [00:37<03:02,  1.08it/s] 16%|█▌        | 38/234 [00:38<03:01,  1.08it/s] 17%|█▋        | 39/234 [00:39<03:01,  1.08it/s] 17%|█▋        | 40/234 [00:40<03:00,  1.08it/s] 18%|█▊        | 41/234 [00:41<02:59,  1.07it/s] 18%|█▊        | 42/234 [00:42<02:58,  1.07it/s] 18%|█▊        | 43/234 [00:43<02:57,  1.08it/s] 19%|█▉        | 44/234 [00:44<02:56,  1.08it/s] 19%|█▉        | 45/234 [00:44<02:55,  1.07it/s] 20%|█▉        | 46/234 [00:45<02:55,  1.07it/s] 20%|██        | 47/234 [00:46<02:54,  1.07it/s] 21%|██        | 48/234 [00:47<02:53,  1.07it/s] 21%|██        | 49/234 [00:48<02:52,  1.07it/s] 21%|██▏       | 50/234 [00:49<02:51,  1.07it/s] 22%|██▏       | 51/234 [00:50<02:50,  1.07it/s] 22%|██▏       | 52/234 [00:51<02:49,  1.07it/s] 23%|██▎       | 53/234 [00:52<02:48,  1.07it/s] 23%|██▎       | 54/234 [00:53<02:47,  1.07it/s] 24%|██▎       | 55/234 [00:54<02:46,  1.07it/s] 24%|██▍       | 56/234 [00:55<02:46,  1.07it/s] 24%|██▍       | 57/234 [00:56<02:45,  1.07it/s] 25%|██▍       | 58/234 [00:57<02:44,  1.07it/s] 25%|██▌       | 59/234 [00:58<02:43,  1.07it/s] 26%|██▌       | 60/234 [00:58<02:42,  1.07it/s] 26%|██▌       | 61/234 [00:59<02:41,  1.07it/s] 26%|██▋       | 62/234 [01:00<02:40,  1.07it/s] 27%|██▋       | 63/234 [01:01<02:39,  1.07it/s] 27%|██▋       | 64/234 [01:02<02:38,  1.07it/s] 28%|██▊       | 65/234 [01:03<02:37,  1.07it/s] 28%|██▊       | 66/234 [01:04<02:36,  1.07it/s] 29%|██▊       | 67/234 [01:05<02:36,  1.07it/s] 29%|██▉       | 68/234 [01:06<02:35,  1.07it/s] 29%|██▉       | 69/234 [01:07<02:34,  1.07it/s] 30%|██▉       | 70/234 [01:08<02:33,  1.07it/s] 30%|███       | 71/234 [01:09<02:32,  1.07it/s] 31%|███       | 72/234 [01:10<02:31,  1.07it/s] 31%|███       | 73/234 [01:11<02:30,  1.07it/s] 32%|███▏      | 74/234 [01:12<02:29,  1.07it/s] 32%|███▏      | 75/234 [01:13<02:28,  1.07it/s] 32%|███▏      | 76/234 [01:13<02:27,  1.07it/s] 33%|███▎      | 77/234 [01:14<02:26,  1.07it/s] 33%|███▎      | 78/234 [01:15<02:19,  1.12it/s] 34%|███▍      | 79/234 [01:16<02:20,  1.10it/s] 34%|███▍      | 80/234 [01:17<02:20,  1.09it/s] 35%|███▍      | 81/234 [01:18<02:20,  1.09it/s] 35%|███▌      | 82/234 [01:19<02:20,  1.08it/s] 35%|███▌      | 83/234 [01:20<02:20,  1.08it/s] 36%|███▌      | 84/234 [01:21<02:19,  1.07it/s] 36%|███▋      | 85/234 [01:22<02:18,  1.07it/s] 37%|███▋      | 86/234 [01:23<02:18,  1.07it/s] 37%|███▋      | 87/234 [01:24<02:17,  1.07it/s] 38%|███▊      | 88/234 [01:25<02:16,  1.07it/s] 38%|███▊      | 89/234 [01:25<02:15,  1.07it/s] 38%|███▊      | 90/234 [01:26<02:14,  1.07it/s] 39%|███▉      | 91/234 [01:27<02:13,  1.07it/s] 39%|███▉      | 92/234 [01:28<02:12,  1.07it/s] 40%|███▉      | 93/234 [01:29<02:12,  1.07it/s] 40%|████      | 94/234 [01:30<02:11,  1.07it/s] 41%|████      | 95/234 [01:31<02:10,  1.07it/s] 41%|████      | 96/234 [01:32<02:09,  1.07it/s] 41%|████▏     | 97/234 [01:33<02:08,  1.07it/s] 42%|████▏     | 98/234 [01:34<02:07,  1.07it/s] 42%|████▏     | 99/234 [01:35<02:06,  1.07it/s] 43%|████▎     | 100/234 [01:36<02:05,  1.07it/s] 43%|████▎     | 101/234 [01:37<02:04,  1.07it/s] 44%|████▎     | 102/234 [01:38<02:03,  1.07it/s] 44%|████▍     | 103/234 [01:39<02:02,  1.07it/s] 44%|████▍     | 104/234 [01:40<02:01,  1.07it/s] 45%|████▍     | 105/234 [01:40<02:00,  1.07it/s] 45%|████▌     | 106/234 [01:41<01:59,  1.07it/s] 46%|████▌     | 107/234 [01:42<01:58,  1.07it/s] 46%|████▌     | 108/234 [01:43<01:57,  1.07it/s] 47%|████▋     | 109/234 [01:44<01:57,  1.07it/s] 47%|████▋     | 110/234 [01:45<01:56,  1.07it/s] 47%|████▋     | 111/234 [01:46<01:55,  1.07it/s] 48%|████▊     | 112/234 [01:47<01:54,  1.07it/s] 48%|████▊     | 113/234 [01:48<01:53,  1.07it/s] 49%|████▊     | 114/234 [01:49<01:52,  1.07it/s] 49%|████▉     | 115/234 [01:50<01:51,  1.07it/s] 50%|████▉     | 116/234 [01:51<01:50,  1.07it/s] 50%|█████     | 117/234 [01:52<01:49,  1.07it/s] 50%|█████     | 118/234 [01:53<01:48,  1.07it/s] 51%|█████     | 119/234 [01:54<01:47,  1.07it/s] 51%|█████▏    | 120/234 [01:54<01:46,  1.07it/s] 52%|█████▏    | 121/234 [01:55<01:45,  1.07it/s] 52%|█████▏    | 122/234 [01:56<01:44,  1.07it/s] 53%|█████▎    | 123/234 [01:57<01:43,  1.07it/s] 53%|█████▎    | 124/234 [01:58<01:42,  1.07it/s] 53%|█████▎    | 125/234 [01:59<01:42,  1.07it/s] 54%|█████▍    | 126/234 [02:00<01:41,  1.07it/s] 54%|█████▍    | 127/234 [02:01<01:40,  1.07it/s] 55%|█████▍    | 128/234 [02:02<01:39,  1.07it/s] 55%|█████▌    | 129/234 [02:03<01:38,  1.07it/s] 56%|█████▌    | 130/234 [02:04<01:37,  1.07it/s] 56%|█████▌    | 131/234 [02:05<01:36,  1.07it/s] 56%|█████▋    | 132/234 [02:06<01:35,  1.07it/s] 57%|█████▋    | 133/234 [02:07<01:34,  1.07it/s] 57%|█████▋    | 134/234 [02:08<01:33,  1.07it/s] 58%|█████▊    | 135/234 [02:09<01:32,  1.07it/s] 58%|█████▊    | 136/234 [02:09<01:31,  1.07it/s] 59%|█████▊    | 137/234 [02:10<01:30,  1.07it/s] 59%|█████▉    | 138/234 [02:11<01:29,  1.07it/s] 59%|█████▉    | 139/234 [02:12<01:29,  1.07it/s] 60%|█████▉    | 140/234 [02:13<01:28,  1.07it/s] 60%|██████    | 141/234 [02:14<01:27,  1.07it/s] 61%|██████    | 142/234 [02:15<01:26,  1.07it/s] 61%|██████    | 143/234 [02:16<01:25,  1.07it/s] 62%|██████▏   | 144/234 [02:17<01:24,  1.07it/s] 62%|██████▏   | 145/234 [02:18<01:23,  1.07it/s] 62%|██████▏   | 146/234 [02:19<01:22,  1.07it/s] 63%|██████▎   | 147/234 [02:20<01:21,  1.07it/s] 63%|██████▎   | 148/234 [02:21<01:20,  1.07it/s] 64%|██████▎   | 149/234 [02:22<01:19,  1.07it/s] 64%|██████▍   | 150/234 [02:23<01:18,  1.07it/s] 65%|██████▍   | 151/234 [02:24<01:17,  1.07it/s] 65%|██████▍   | 152/234 [02:24<01:16,  1.07it/s] 65%|██████▌   | 153/234 [02:25<01:15,  1.07it/s] 66%|██████▌   | 154/234 [02:26<01:14,  1.07it/s] 66%|██████▌   | 155/234 [02:27<01:14,  1.07it/s] 67%|██████▋   | 156/234 [02:28<01:09,  1.12it/s] 67%|██████▋   | 157/234 [02:29<01:09,  1.10it/s] 68%|██████▊   | 158/234 [02:30<01:09,  1.09it/s] 68%|██████▊   | 159/234 [02:31<01:09,  1.08it/s] 68%|██████▊   | 160/234 [02:32<01:08,  1.08it/s] 69%|██████▉   | 161/234 [02:33<01:07,  1.07it/s] 69%|██████▉   | 162/234 [02:34<01:07,  1.07it/s] 70%|██████▉   | 163/234 [02:35<01:06,  1.07it/s] 70%|███████   | 164/234 [02:36<01:05,  1.07it/s] 71%|███████   | 165/234 [02:37<01:04,  1.07it/s] 71%|███████   | 166/234 [02:37<01:03,  1.07it/s] 71%|███████▏  | 167/234 [02:38<01:02,  1.07it/s] 72%|███████▏  | 168/234 [02:39<01:01,  1.07it/s] 72%|███████▏  | 169/234 [02:40<01:00,  1.07it/s] 73%|███████▎  | 170/234 [02:41<00:59,  1.07it/s] 73%|███████▎  | 171/234 [02:42<00:59,  1.07it/s] 74%|███████▎  | 172/234 [02:43<00:58,  1.07it/s] 74%|███████▍  | 173/234 [02:44<00:57,  1.07it/s] 74%|███████▍  | 174/234 [02:45<00:56,  1.07it/s] 75%|███████▍  | 175/234 [02:46<00:55,  1.07it/s] 75%|███████▌  | 176/234 [02:47<00:54,  1.07it/s] 76%|███████▌  | 177/234 [02:48<00:53,  1.07it/s] 76%|███████▌  | 178/234 [02:49<00:52,  1.07it/s] 76%|███████▋  | 179/234 [02:50<00:51,  1.07it/s] 77%|███████▋  | 180/234 [02:51<00:50,  1.07it/s] 77%|███████▋  | 181/234 [02:52<00:49,  1.07it/s] 78%|███████▊  | 182/234 [02:52<00:48,  1.07it/s] 78%|███████▊  | 183/234 [02:53<00:47,  1.07it/s] 79%|███████▊  | 184/234 [02:54<00:46,  1.07it/s] 79%|███████▉  | 185/234 [02:55<00:45,  1.07it/s] 79%|███████▉  | 186/234 [02:56<00:45,  1.07it/s] 80%|███████▉  | 187/234 [02:57<00:44,  1.07it/s] 80%|████████  | 188/234 [02:58<00:43,  1.07it/s] 81%|████████  | 189/234 [02:59<00:42,  1.07it/s] 81%|████████  | 190/234 [03:00<00:41,  1.07it/s] 82%|████████▏ | 191/234 [03:01<00:40,  1.07it/s] 82%|████████▏ | 192/234 [03:02<00:39,  1.07it/s] 82%|████████▏ | 193/234 [03:03<00:38,  1.07it/s] 83%|████████▎ | 194/234 [03:04<00:37,  1.07it/s] 83%|████████▎ | 195/234 [03:05<00:36,  1.07it/s] 84%|████████▍ | 196/234 [03:06<00:35,  1.07it/s] 84%|████████▍ | 197/234 [03:07<00:34,  1.07it/s] 85%|████████▍ | 198/234 [03:07<00:33,  1.07it/s] 85%|████████▌ | 199/234 [03:08<00:32,  1.07it/s] 85%|████████▌ | 200/234 [03:09<00:31,  1.07it/s] 86%|████████▌ | 201/234 [03:10<00:30,  1.07it/s] 86%|████████▋ | 202/234 [03:11<00:30,  1.07it/s] 87%|████████▋ | 203/234 [03:12<00:29,  1.07it/s] 87%|████████▋ | 204/234 [03:13<00:28,  1.07it/s] 88%|████████▊ | 205/234 [03:14<00:27,  1.07it/s] 88%|████████▊ | 206/234 [03:15<00:26,  1.07it/s] 88%|████████▊ | 207/234 [03:16<00:25,  1.07it/s] 89%|████████▉ | 208/234 [03:17<00:24,  1.07it/s] 89%|████████▉ | 209/234 [03:18<00:23,  1.07it/s] 90%|████████▉ | 210/234 [03:19<00:22,  1.07it/s] 90%|█████████ | 211/234 [03:20<00:21,  1.07it/s] 91%|█████████ | 212/234 [03:21<00:20,  1.07it/s] 91%|█████████ | 213/234 [03:22<00:19,  1.07it/s] 91%|█████████▏| 214/234 [03:22<00:18,  1.07it/s] 92%|█████████▏| 215/234 [03:23<00:17,  1.07it/s] 92%|█████████▏| 216/234 [03:24<00:16,  1.07it/s] 93%|█████████▎| 217/234 [03:25<00:15,  1.07it/s] 93%|█████████▎| 218/234 [03:26<00:15,  1.07it/s] 94%|█████████▎| 219/234 [03:27<00:14,  1.07it/s] 94%|█████████▍| 220/234 [03:28<00:13,  1.07it/s] 94%|█████████▍| 221/234 [03:29<00:12,  1.07it/s] 95%|█████████▍| 222/234 [03:30<00:11,  1.07it/s] 95%|█████████▌| 223/234 [03:31<00:10,  1.07it/s] 96%|█████████▌| 224/234 [03:32<00:09,  1.07it/s] 96%|█████████▌| 225/234 [03:33<00:08,  1.07it/s] 97%|█████████▋| 226/234 [03:34<00:07,  1.07it/s] 97%|█████████▋| 227/234 [03:35<00:06,  1.07it/s] 97%|█████████▋| 228/234 [03:36<00:05,  1.07it/s] 98%|█████████▊| 229/234 [03:37<00:04,  1.07it/s] 98%|█████████▊| 230/234 [03:37<00:03,  1.07it/s] 99%|█████████▊| 231/234 [03:38<00:02,  1.07it/s] 99%|█████████▉| 232/234 [03:39<00:01,  1.07it/s]100%|█████████▉| 233/234 [03:40<00:00,  1.07it/s]100%|██████████| 234/234 [03:41<00:00,  1.12it/s]WARNING:root:Saving quantized model
                                                 100%|██████████| 234/234 [03:43<00:00,  1.12it/s]100%|██████████| 234/234 [03:43<00:00,  1.05it/s]
{'train_runtime': 223.2572, 'train_samples_per_second': 33.459, 'train_steps_per_second': 1.048, 'train_loss': 0.7166249201847956, 'epoch': 3.0}
Evaluating model...
  0%|          | 0/35 [00:00<?, ?it/s]  6%|▌         | 2/35 [00:00<00:01, 18.40it/s] 11%|█▏        | 4/35 [00:00<00:02, 11.64it/s] 17%|█▋        | 6/35 [00:00<00:02, 10.41it/s] 23%|██▎       | 8/35 [00:00<00:02,  9.92it/s] 29%|██▊       | 10/35 [00:00<00:02,  9.62it/s] 34%|███▍      | 12/35 [00:01<00:02,  9.48it/s] 37%|███▋      | 13/35 [00:01<00:02,  9.44it/s] 40%|████      | 14/35 [00:01<00:02,  9.41it/s] 43%|████▎     | 15/35 [00:01<00:02,  9.36it/s] 46%|████▌     | 16/35 [00:01<00:02,  9.32it/s] 49%|████▊     | 17/35 [00:01<00:01,  9.27it/s] 51%|█████▏    | 18/35 [00:01<00:01,  9.26it/s] 54%|█████▍    | 19/35 [00:01<00:01,  9.25it/s] 57%|█████▋    | 20/35 [00:02<00:01,  9.23it/s] 60%|██████    | 21/35 [00:02<00:01,  9.23it/s] 63%|██████▎   | 22/35 [00:02<00:01,  9.23it/s] 66%|██████▌   | 23/35 [00:02<00:01,  9.23it/s] 69%|██████▊   | 24/35 [00:02<00:01,  9.22it/s] 71%|███████▏  | 25/35 [00:02<00:01,  9.23it/s] 74%|███████▍  | 26/35 [00:02<00:00,  9.24it/s] 77%|███████▋  | 27/35 [00:02<00:00,  9.19it/s] 80%|████████  | 28/35 [00:02<00:00,  9.20it/s] 83%|████████▎ | 29/35 [00:03<00:00,  9.20it/s] 86%|████████▌ | 30/35 [00:03<00:00,  9.19it/s] 89%|████████▊ | 31/35 [00:03<00:00,  9.21it/s] 91%|█████████▏| 32/35 [00:03<00:00,  9.21it/s] 94%|█████████▍| 33/35 [00:03<00:00,  9.20it/s] 97%|█████████▋| 34/35 [00:03<00:00,  9.23it/s]WARNING:evaluate.loading:Using the latest cached version of the module from /scratch/cjm1n19/huggingface/modules/evaluate_modules/metrics/evaluate-metric--glue/05234ba7acc44554edcca0978db5fa3bc600eeee66229abe79ff9887eacaf3ed (last modified on Wed Apr 23 17:11:50 2025) since it couldn't be found locally at evaluate-metric--glue, or remotely on the Hugging Face Hub.
100%|██████████| 35/35 [00:05<00:00,  6.40it/s]
WARNING:root:Saving quantized model
Validation Matched Results: {'eval_loss': 0.6835674047470093, 'eval_accuracy': 0.6353790613718412, 'eval_runtime': 5.7453, 'eval_samples_per_second': 48.214, 'eval_steps_per_second': 6.092, 'epoch': 3.0}
Saving final fine-tuned model...
Process completed!
==============================================================================
Running epilogue script on pink58.

Submit time  : 2025-04-30T13:14:46
Start time   : 2025-04-30T13:14:46
End time     : 2025-04-30T13:20:38
Elapsed time : 00:05:52 (Timelimit=10:00:00)

Job ID: 7348485
Cluster: i5
User/Group: cjm1n19/fp
State: COMPLETED (exit code 0)
Nodes: 1
Cores per node: 16
CPU Utilized: 00:04:11
CPU Efficiency: 4.46% of 01:33:52 core-walltime
Job Wall-clock time: 00:05:52
Memory Utilized: 9.17 GB
Memory Efficiency: 0.00% of 16.00 B

