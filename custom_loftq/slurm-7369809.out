Running SLURM prolog script on pink51.cluster.local
===============================================================================
Job started on Mon May  5 21:44:19 BST 2025
Job ID          : 7369809
Job name        : AdaNF_MNLI
WorkDir         : /mainfs/lyceum/be1g21/LoftQ_Private/custom_loftq
Command         : /mainfs/lyceum/be1g21/LoftQ_Private/custom_loftq/AdaNF_mnli.sh
Partition       : lyceum
Num hosts       : 1
Num cores       : 16
Num of tasks    : 1
Hosts allocated : pink51
Job Output Follows ...
===============================================================================
Starting AdaNF MNLI run...
The following values were not passed to `accelerate launch` and had defaults used instead:
	`--num_machines` was set to a value of `1`
	`--dynamo_backend` was set to a value of `'no'`
To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.
[2025-05-05 21:46:01,913] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)
WARNING:root:Loading raw dataset
Using the latest cached version of the dataset since glue couldn't be found on the Hugging Face Hub
WARNING:datasets.load:Using the latest cached version of the dataset since glue couldn't be found on the Hugging Face Hub
Traceback (most recent call last):
  File "/mainfs/lyceum/be1g21/LoftQ_Private/custom_loftq/run_loftq.py", line 14, in <module>
    raw_data = load_raw_dataset(data_args.data_name, data_args.task_name)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/mainfs/lyceum/be1g21/LoftQ_Private/custom_loftq/utils.py", line 13, in load_raw_dataset
    raw_data = load_dataset(dataset_name, task_name)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/lyceum/be1g21/.conda/envs/loftq/lib/python3.12/site-packages/datasets/load.py", line 2062, in load_dataset
    builder_instance = load_dataset_builder(
                       ^^^^^^^^^^^^^^^^^^^^^
  File "/lyceum/be1g21/.conda/envs/loftq/lib/python3.12/site-packages/datasets/load.py", line 1819, in load_dataset_builder
    builder_instance: DatasetBuilder = builder_cls(
                                       ^^^^^^^^^^^^
  File "/lyceum/be1g21/.conda/envs/loftq/lib/python3.12/site-packages/datasets/packaged_modules/cache/cache.py", line 124, in __init__
    config_name, version, hash = _find_hash_in_cache(
                                 ^^^^^^^^^^^^^^^^^^^^
  File "/lyceum/be1g21/.conda/envs/loftq/lib/python3.12/site-packages/datasets/packaged_modules/cache/cache.py", line 64, in _find_hash_in_cache
    raise ValueError(
ValueError: Couldn't find cache for glue for config 'mnli'
Available configs in the cache: ['rte']
Traceback (most recent call last):
  File "/lyceum/be1g21/.conda/envs/loftq/bin/accelerate", line 8, in <module>
    sys.exit(main())
             ^^^^^^
  File "/lyceum/be1g21/.conda/envs/loftq/lib/python3.12/site-packages/accelerate/commands/accelerate_cli.py", line 50, in main
    args.func(args)
  File "/lyceum/be1g21/.conda/envs/loftq/lib/python3.12/site-packages/accelerate/commands/launch.py", line 1213, in launch_command
    simple_launcher(args)
  File "/lyceum/be1g21/.conda/envs/loftq/lib/python3.12/site-packages/accelerate/commands/launch.py", line 795, in simple_launcher
    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)
subprocess.CalledProcessError: Command '['/lyceum/be1g21/.conda/envs/loftq/bin/python', 'run_loftq.py', '--model_name_or_path', 'microsoft/deberta-v3-base', '--data_name', 'glue', '--task_name', 'mnli', '--loftq', '--quant_method', 'adanf', '--int_bit', '2', '--adanf_pnorm', '3', '--adanf_grid_size', '10', '--reduced_rank', '32', '--num_iter', '2', '--gradient_accumulation_steps', '1', '--per_device_train_batch_size', '32', '--learning_rate', '1e-4', '--num_train_epochs', '5', '--output_dir', '/scratch/be1g21/trained_models/adanf_mnli_b2_r32_lr1e-4_e5', '--logging_dir', '/scratch/be1g21/trainer_outputs/adanf_mnli_logs', '--logging_steps', '50', '--true_quantization']' returned non-zero exit status 1.
==============================================================================
Running epilogue script on pink51.

Submit time  : 2025-05-05T21:43:51
Start time   : 2025-05-05T21:44:19
End time     : 2025-05-05T21:46:21
Elapsed time : 00:02:02 (Timelimit=2-00:00:00)

Job ID: 7369809
Cluster: i5
User/Group: be1g21/fp
State: FAILED (exit code 1)
Nodes: 1
Cores per node: 16
CPU Utilized: 00:00:18
CPU Efficiency: 0.92% of 00:32:32 core-walltime
Job Wall-clock time: 00:02:02
Memory Utilized: 1.09 GB
Memory Efficiency: 0.00% of 16.00 B

