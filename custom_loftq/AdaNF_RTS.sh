#!/bin/bash -l
#SBATCH -p lyceum                 # ðŸ”§ cluster/partition
#SBATCH --nodes=1
#SBATCH -c 16
#SBATCH --gres=gpu:1              # 1â€¯GPU is enough for DeBERTaâ€‘v3â€‘base
#SBATCH --mem=48G                 # â†“ (RTE is tiny; 48Â GB is plenty)
#SBATCH --time=04:00:00           # â†“ training converges in <â€¯2â€¯h on A100
#SBATCH --mail-type=ALL
#SBATCH --mail-user=be1g21@soton.ac.uk

module load conda/py3-latest
conda activate loftq            # environment that has HFÂ +Â accelerateÂ +Â bitsandbytes

# ----------- Hyperâ€‘params -------------------------------------------------
MODEL="microsoft/deberta-v3-base"   # ðŸ”§ same backbone as LoftQ paper
TASK="rte"                          # GLUE RTE
BITS=2                              # 2â€‘bit quant
PNORM=3                             # AdaNF L3
GRID=12                             # 12 offset candidates / block
RANK=32                             # LoRA rank
ITER=2                              # LoftQ alternations
BATCH=32
ACCUM=1
# -------------------------------------------------------------------------

accelerate launch \
  --num_processes 1 \
  --mixed_precision no \
  run_loftq.py \
  --model_name_or_path "${MODEL}" \
  --data_name glue --task_name "${TASK}" \
  --loftq \
  --quant_method adanf \
  --int_bit "${BITS}" \
  --adanf_pnorm "${PNORM}" \
  --adanf_grid_size "${GRID}" \
  --reduced_rank "${RANK}" \
  --num_iter "${ITER}" \
  --gradient_accumulation_steps "${ACCUM}" \
  --per_device_train_batch_size "${BATCH}" \
  --true_quantization
